{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT IMPORTANCE LIBRARY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import threading\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO OPEN N-BROWSERS BASED ON NUM OF THREADS\n",
    "def open_multi_browsers(n_page):\n",
    "    drivers = []\n",
    "    for _ in range(n_page):\n",
    "        driver = webdriver.Chrome()\n",
    "        drivers.append(driver)\n",
    "    return drivers\n",
    "\n",
    "#FUNCTION TO LOAD LINK - USE ON load_multi_browsers\n",
    "def load_multi_pages(driver, n):\n",
    "    driver.maximize_window()\n",
    "    link = f'https://tiki.vn/sach-truyen-tieng-viet/c316?page={n}'\n",
    "    driver.get(link)\n",
    "    sleep(3)\n",
    "\n",
    "#FUNCTION TO LOAD THREADING OF MULTI BROWSERS\n",
    "def load_multi_browsers(drivers, idx_page):\n",
    "    for driver, page in zip(drivers, idx_page):\n",
    "        t = threading.Thread(target = load_multi_pages, args = (driver, page))\n",
    "        t.start()\n",
    "\n",
    "#FUNCTION TO TAKE DATA FROM THREADING AND SAVE IT ON QUEUE - USE ON RunInParallel\n",
    "def get_data(driver, que):\n",
    "    try:\n",
    "        prod_links_elems = driver.find_elements(By.CSS_SELECTOR, '.style__ProductLink-sc-7xd6qw-2.fHwskZ.product-item')\n",
    "        prod_links = [i.get_attribute('href') for i in prod_links_elems]\n",
    "    except TimeoutException:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        element_to_wait = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.style__ProductLink-sc-7xd6qw-2.fHwskZ.product-item')))\n",
    "        prod_links_elems = driver.find_elements(By.CSS_SELECTOR, '.style__ProductLink-sc-7xd6qw-2.fHwskZ.product-item')\n",
    "        prod_links = [i.get_attribute('href') for i in prod_links_elems]\n",
    "\n",
    "    page_prod_features = []\n",
    "\n",
    "    for prod_link in prod_links:\n",
    "        driver.get(prod_link)\n",
    "        sleep(2)\n",
    "        driver.maximize_window()\n",
    "        scroll_iterations = 10\n",
    "        scroll_amount = 300\n",
    "        scroll_interval = 0.2 \n",
    "\n",
    "        for _ in range(scroll_iterations):\n",
    "            driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_amount)\n",
    "            sleep(scroll_interval)\n",
    "\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            element_to_wait = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.btn-more')))\n",
    "            element_to_wait.click()\n",
    "        except TimeoutException:\n",
    "            print('Not btn-more')\n",
    "\n",
    "        try:\n",
    "            category_elems = driver.find_elements(By.CSS_SELECTOR, '.Breadcrumb__Wrapper-sc-1r2fjia-0.gsoENx .breadcrumb-item')\n",
    "            category = [i.text for i in category_elems]\n",
    "        except NoSuchElementException:\n",
    "            category = np.nan\n",
    "\n",
    "        try:\n",
    "            img_elem = driver.find_element(By.CSS_SELECTOR, '.image-frame')\n",
    "            img = img_elem.find_element(By.TAG_NAME, 'img').get_attribute('srcset').split(' ')[0]\n",
    "        except NoSuchElementException:\n",
    "            img = np.nan\n",
    "\n",
    "        try:\n",
    "            price = driver.find_element(By.CSS_SELECTOR, '.product-price__current-price').text\n",
    "        except NoSuchElementException:\n",
    "            price = np.nan\n",
    "\n",
    "        try:\n",
    "            discount = driver.find_element(By.CSS_SELECTOR, '.product-price__discount-rate').text\n",
    "        except NoSuchElementException:\n",
    "            discount = np.nan\n",
    "\n",
    "        try:\n",
    "            sale_quantities = driver.find_element(By.CSS_SELECTOR, '.styles__StyledQuantitySold-sc-1swui9f-3.bExXAB').text\n",
    "        except NoSuchElementException:\n",
    "            sale_quantities = np.nan\n",
    "\n",
    "        try:\n",
    "            rating = driver.find_element(By.CSS_SELECTOR, '.styles__StyledReview-sc-1swui9f-1.dXPbue').text\n",
    "        except NoSuchElementException:\n",
    "            rating = np.nan\n",
    "\n",
    "        \n",
    "\n",
    "        info_elems = driver.find_elements(By.CSS_SELECTOR, '.WidgetTitle__WidgetContainerStyled-sc-1ikmn8z-0.iHMNqO')\n",
    "        for i in info_elems:\n",
    "            try:\n",
    "                title = i.find_element(By.CSS_SELECTOR, '.WidgetTitle__WidgetTitleStyled-sc-1ikmn8z-1.eaKcuo').text\n",
    "                print(title)\n",
    "                if title == 'Thông tin chi tiết':\n",
    "                    info_row = i.find_elements(By.CSS_SELECTOR, '.WidgetTitle__WidgetContentStyled-sc-1ikmn8z-2.jMQTPW')\n",
    "                    info = [i.text.split('\\n') for i in info_row]\n",
    "                    print('Success collect info')\n",
    "                elif title == 'Mô tả sản phẩm':\n",
    "                    describe = i.find_element(By.CSS_SELECTOR, '.style__Wrapper-sc-13sel60-0.dGqjau.content').text\n",
    "                    print('Success collect describe')\n",
    "                elif title == 'Thông tin nhà bán':\n",
    "                    seller = i.find_element(By.CSS_SELECTOR, '.seller-name').text.split(' ')[0]\n",
    "                    seller_evaluation_elems = i.find_element(By.CSS_SELECTOR, '.item.review')\n",
    "                    seller_star = seller_evaluation_elems.find_element(By.CSS_SELECTOR, '.title').text\n",
    "                    seller_reviews_quantity = seller_evaluation_elems.find_element(By.CSS_SELECTOR, '.sub-title').text\n",
    "                    seller_follow = i.find_element(By.CSS_SELECTOR, '.item.normal .title').text\n",
    "                    print('Succes collect seller info')\n",
    "            except NoSuchElementException:\n",
    "                print('PASS')\n",
    "\n",
    "        features = [category, img, price, discount, sale_quantities, rating, info, describe, seller, seller_star, seller_reviews_quantity, seller_follow]\n",
    "        page_prod_features.append(features)\n",
    "        \n",
    "    que.put(page_prod_features)\n",
    "\n",
    "#FUNCTION TO RUN THREADING OF TAKE DATA AND RETURN IN A LIST \n",
    "def runInParallbel(func, drivers):\n",
    "    threads = []\n",
    "    que = Queue()\n",
    "\n",
    "    for driver in drivers:\n",
    "        print('--Running--')\n",
    "        t = threading.Thread(target = func, args = (driver, que))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    results = []\n",
    "    while not que.empty():\n",
    "        results.extend(que.get())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN 5 THREADS FOR CRAWL OF 50 PAGES\n",
    "n_page = 5\n",
    "drivers = open_multi_browsers(n_page)\n",
    "idx_page = [i for i in range(1, n_page + 1)]\n",
    "all_data = pd.DataFrame()\n",
    "while idx_page[0] < 50:\n",
    "    load_multi_browsers(drivers, idx_page)\n",
    "    sleep(5)\n",
    "    all_prod_features = runInParallbel(get_data, drivers)\n",
    "    page_df = pd.DataFrame(all_prod_features, columns = ['category', 'img', 'price', 'discount', 'sale_quantities', 'rating', 'info', 'describe', 'seller', 'seller_star', 'seller_reviews_quantity', 'seller_follow'])\n",
    "    all_data = pd.concat([all_data, page_df], axis = 0)\n",
    "    idx_page = [i + 5 for i in idx_page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE IT TO CSV FILE\n",
    "all_data.to_csv('book_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
